{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import time\n",
    "\n",
    "#for mySQL connection\n",
    "import mysql\n",
    "import mysql.connector\n",
    "import pymysql\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "#import pushshift API to get comments from Reddit\n",
    "from pmaw import PushshiftAPI\n",
    "\n",
    "#for envorinment variables\n",
    "import os\n",
    "\n",
    "#regex for removing links\n",
    "import re\n",
    "\n",
    "#for nltk, removing everything but letters\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "#for detecting gibberish(non-english words)\n",
    "from gibberish_detector import detector\n",
    "Detector = detector.create_from_model('Gibberish-Detector\\gibberish-detector.model')\n",
    "\n",
    "#for removing contractions (e.g. don't -> do not)\n",
    "import contractions\n",
    "\n",
    "#for removal of accents\n",
    "import unicodedata\n",
    "\n",
    "#for spacy/lemmatization\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm', disable = ['parser','ner'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cython extension is already loaded. To reload it, use:\n",
      "  %reload_ext cython\n"
     ]
    }
   ],
   "source": [
    "#load cython\n",
    "%load_ext cython\n",
    "#increase number of displayed columns to max\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "#for PRAW - currently not used\n",
    "import praw\n",
    "#the following were set using the following guide - https://github.com/reddit-archive/reddit/wiki/OAuth2\n",
    "r_client_id = os.environ['reddit_client_id']\n",
    "r_secret_key = os.environ['reddit_secret_key']\n",
    "r_username = os.environ['reddit_username']\n",
    "r_password = os.environ['reddit_password']\n",
    "r_app_id = 'Sub_specific_corpus'\n",
    "app_version = 'v0.1'\n",
    "python_version = !python -V\n",
    "r_user_agent = '{}:{}:{} (by u/{})'.format(python_version, r_app_id, app_version, r_username)\n",
    "#import logging #not implemented - https://praw.readthedocs.io/en/stable/getting_started/logging.html\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQL database creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_to_SQL():\n",
    "    return mysql.connector.connect(\n",
    "        host='localhost',\n",
    "        user='root',\n",
    "        passwd='20RUNstackHost',\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_to_db(db_name):\n",
    "    return mysql.connector.connect(\n",
    "        host='localhost',\n",
    "        user='root',\n",
    "        passwd='20RUNstackHost',\n",
    "        database=db_name\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_if_db_exists(cursor, db_name):\n",
    "    cursor.execute(\"SHOW DATABASES LIKE '{}'\".format(db_name))\n",
    "    return bool(cursor.fetchone())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_if_table_exists(cursor, table_name):\n",
    "    cursor.execute(\"SHOW TABLES LIKE '{}'\".format(table_name))\n",
    "    return bool(cursor.fetchone())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_db(cursor, db_name):\n",
    "    cursor.execute(\"CREATE DATABASE {}\".format(db_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_comments_table(cursor):\n",
    "    cursor.execute(\"CREATE TABLE comments (author VARCHAR(40), author_fullname VARCHAR(255),body TEXT, created_utc VARCHAR(255), id VARCHAR(20) PRIMARY KEY, is_submitter VARCHAR(255), link_id VARCHAR(255), parent_id VARCHAR(255), permalink VARCHAR(255), retrieved_on VARCHAR(30), score INT, subreddit VARCHAR(255), subreddit_id VARCHAR(255))\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_table_to_db(db_name, table):\n",
    "    cnx_SQL = connect_to_SQL()\n",
    "    cursor_SQL = cnx_SQL.cursor()\n",
    "    if not check_if_db_exists(cursor_SQL, db_name):\n",
    "        add_db(cursor_SQL, db_name)\n",
    "        print('Database {} created'.format(db_name))\n",
    "    cnx_SQL.commit()\n",
    "    cursor_SQL.close()\n",
    "    cnx_SQL.close()\n",
    "    cnx = connect_to_db(db_name)\n",
    "    cursor = cnx.cursor()\n",
    "    if not check_if_table_exists(cursor, table):\n",
    "        if table == 'comments':\n",
    "            add_comments_table(cursor)\n",
    "            print('Table {} created'.format(table))\n",
    "    cnx.commit()\n",
    "    cursor.close()\n",
    "    cnx.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reddit API (PRAW) - Not used yet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```\n",
    "#lets see if we can use the reddit API\n",
    "reddit = praw.Reddit(\n",
    "    client_id=r_client_id,\n",
    "    client_secret=r_secret_key,\n",
    "    user_agent=r_user_agent,\n",
    "    username=r_username,\n",
    "    password=r_password    \n",
    ")\n",
    "subreddit = reddit.subreddit('askreddit')\n",
    "subreddit.comments(limit=100)\n",
    "for i in subreddit.comments(limit=10):\n",
    "    print(i.body)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PushShift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize the pushshift pmaw API\n",
    "api = PushshiftAPI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_comments_df(df):\n",
    "    '''\n",
    "    Cleans the comments dataframe by removing columns that are not needed\n",
    "    '''\n",
    "    return df.drop(columns=['all_awardings', 'associated_award', 'author_flair_background_color', 'author_flair_css_class','author_flair_richtext', 'author_flair_template_id', 'author_flair_text',\\\n",
    "        'author_flair_text_color','author_flair_type', 'author_patreon_flair', 'author_premium', 'awarders', 'collapsed_because_crowd_control', 'comment_type', 'gildings','locked','no_follow',\\\n",
    "        'send_replies','stickied','top_awarded_type', 'total_awards_received', 'treatment_tags', 'archived', 'body_sha1', 'can_gild',  'collapsed', 'collapsed_reason', 'controversiality',\\\n",
    "        'distinguished','gilded','score_hidden','subreddit_name_prefixed','subreddit_type','author_cakeday','unrepliable_reason', 'collapsed_reason_code', 'retrieved_utc', 'edited'], axis=1, errors='ignore')       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pushshift_comments(subreddit, before, after, limit):\n",
    "    '''\n",
    "    Gets comments from the pushshift API\n",
    "    '''\n",
    "    return api.search_comments(subreddit=subreddit, metadata=True, before=before, after=after, limit=limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_pushshift_to_df(comments):\n",
    "    '''\n",
    "    Converts the comments from the pushshift API to a dataframe\n",
    "    '''\n",
    "    comment_list = [c for c in comments]\n",
    "    comments_df = pd.DataFrame(comment_list)\n",
    "    return comments_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_df_to_db(df, db_name, table):\n",
    "    '''\n",
    "    Sends the dataframe to the database\n",
    "    '''\n",
    "    add_table_to_db(db_name, table)\n",
    "    engine = create_engine('mysql+pymysql://root:20RUNstackHost@localhost/{}'.format(db_name))\n",
    "    df.to_sql(table, con = engine, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comments(subreddit, before, after, limit=None):\n",
    "    '''\n",
    "    Gets comments from the pushshift API and sends them to the database\n",
    "    '''\n",
    "    comments = get_pushshift_comments(subreddit, before, after, limit)\n",
    "    comments_df = from_pushshift_to_df(comments)\n",
    "    comments_df = clean_comments_df(comments_df)\n",
    "    send_df_to_db(comments_df, subreddit, 'comments')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### records of pulling from push-shift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "#getting all the data at once is a bad idea lets build a loop that does it 5000 dt units at a time\n",
    "#original_start_date = int(dt.datetime(2021, 1, 1, 0, 0).timestamp())\n",
    "#original_end_date = int(dt.datetime(2022, 1, 1, 0, 0).timestamp())\n",
    "while start_date<end_date:\n",
    "    get_comments('askreddit', end_date, end_date-5000)\n",
    "    end_date-=5000\n",
    "\n",
    "#ran it 500 minutes, got around 2 million comments, moved on to the next subreddit\n",
    "#ending end_date = 1640203500 (2021-12-22-8-05-00)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "#getting all the data at once is a bad idea lets build a loop that does it 10000 dt units at a time\n",
    "#original_start_date = int(dt.datetime(2021, 1, 1, 0, 0).timestamp())\n",
    "#original_end_date = int(dt.datetime(2022, 1, 1, 0, 0).timestamp())\n",
    "start_date = int(dt.datetime(2021, 1, 1, 0, 0).timestamp())\n",
    "end_date = 1638904006\n",
    "count = 0\n",
    "\n",
    "while start_date<end_date:\n",
    "    get_comments('askmen', end_date, end_date-10000)\n",
    "    end_date-=10000\n",
    "    count += 1\n",
    "    print(\"{} iterations last end_date ={}\".format(count, end_date))\n",
    "#ran it for 623 min, got around 2.5 mil comments, moved on to the next subreddit\n",
    "#ending end_date = 1626774006, (2021-07-20-9-40-06)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "#getting all the data at once is a bad idea lets build a loop that does it 20000 dt units at a time\n",
    "#original_start_date = int(dt.datetime(2021, 1, 1, 0, 0).timestamp())\n",
    "#original_end_date = int(dt.datetime(2022, 1, 1, 0, 0).timestamp())\n",
    "start_date = int(dt.datetime(2021, 1, 1, 0, 0).timestamp())\n",
    "end_date = int(dt.datetime(2022, 1, 1, 0, 0).timestamp())\n",
    "count = 0\n",
    "\n",
    "while start_date<end_date:\n",
    "    get_comments('askwomen', end_date, end_date-20000)\n",
    "    end_date-=20000\n",
    "    count += 1\n",
    "    print(\"{} iterations last end_date ={}\".format(count, end_date))\n",
    "\n",
    "#ran it for 485min, got 1534382 comments, moved on to the next subreddit\n",
    "#ending end_date = 1609484000 #APROX Jan 1st 2021 ran to completion \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "#original_start_date = int(dt.datetime(2021, 1, 1, 0, 0).timestamp())\n",
    "#original_end_date = int(dt.datetime(2022, 1, 1, 0, 0).timestamp())\n",
    "start_date = int(dt.datetime(2021, 1, 1, 0, 0).timestamp())\n",
    "end_date = int(dt.datetime(2022, 1, 1, 0, 0).timestamp())\n",
    "count = 0\n",
    "\n",
    "while start_date<end_date:\n",
    "    get_comments('dota2', end_date, end_date-20000)\n",
    "    end_date-=20000\n",
    "    count += 1\n",
    "    print(\"{} iterations, last end_date ={}\".format(count, end_date))\n",
    "#ran it to completion in 518 min, got 1740932 comments\n",
    "#ending end_date = 1609484000 #APROX Jan 1st 2021 ran to completion\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "#getting all the data at once is a bad idea lets build a loop that does it 10000 dt units at a time\n",
    "#original_start_date = int(dt.datetime(2021, 1, 1, 0, 0).timestamp())\n",
    "#original_end_date = int(dt.datetime(2022, 1, 1, 0, 0).timestamp())\n",
    "start_date = int(dt.datetime(2021, 1, 1, 0, 0).timestamp())\n",
    "end_date = 1618374000\n",
    "count = 0\n",
    "\n",
    "while start_date<end_date:\n",
    "    get_comments('askscience', end_date, end_date-86400)\n",
    "    end_date-=86400\n",
    "    count += 1\n",
    "    print(\"{} iterations, last end_date ={}\".format(count, end_date))\n",
    "#ran it three times, \n",
    "#stopped it at 2105 iterations, 430 min, 217326 records?, seemed low last end_date =1619974000 to increase step size from 10k to 50k\n",
    "#stopped it again at 32 iterations, 7 min, 230515 records, and decided to just increase step size to 1 day. end_date = 1618374000\n",
    "#ran to completion 3rd time 103 iterations, 23 min, 294504 records, end_date = 1609474800\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "#getting all the data at once is a bad idea lets build a loop that does it 86400 dt units at a time (1 day)\n",
    "#original_start_date = int(dt.datetime(2021, 1, 1, 0, 0).timestamp())\n",
    "#original_end_date = int(dt.datetime(2022, 1, 1, 0, 0).timestamp())\n",
    "start_date = int(dt.datetime(2021, 1, 1, 0, 0).timestamp())\n",
    "end_date = int(dt.datetime(2022, 1, 1, 0, 0).timestamp())\n",
    "count = 0\n",
    "\n",
    "while start_date<end_date:\n",
    "    get_comments('politics', end_date, end_date-86400)\n",
    "    end_date-=86400\n",
    "    count += 1\n",
    "    print(\"{} iterations, last end_date ={}\".format(count, end_date))\n",
    "#stopped it after 751 min, 94 iterations, end_date = 1632902400, total records 2,863,608\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "#getting all the data at once is a bad idea lets build a loop that does it 86400 dt units at a time (1 day)\n",
    "#original_start_date = int(dt.datetime(2021, 1, 1, 0, 0).timestamp())\n",
    "#original_end_date = int(dt.datetime(2022, 1, 1, 0, 0).timestamp())\n",
    "start_date = int(dt.datetime(2021, 1, 1, 0, 0).timestamp())\n",
    "end_date = int(dt.datetime(2022, 1, 1, 0, 0).timestamp())\n",
    "count = 0\n",
    "\n",
    "while start_date<end_date:\n",
    "    get_comments('TwoXChromosomes', end_date, end_date-86400)\n",
    "    end_date-=86400\n",
    "    count += 1\n",
    "    print(\"{} iterations, last end_date ={}\".format(count, end_date))\n",
    "#ran for 410 min to completion, got 1,619,521 comments, last end_date =1609488000\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "#getting all the data at once is a bad idea lets build a loop that does it 86400 dt units at a time (1 day)\n",
    "#original_start_date = int(dt.datetime(2021, 1, 1, 0, 0).timestamp())\n",
    "#original_end_date = int(dt.datetime(2022, 1, 1, 0, 0).timestamp())\n",
    "start_date = int(dt.datetime(2021, 1, 1, 0, 0).timestamp())\n",
    "end_date = 1629878400\n",
    "count = 0\n",
    "\n",
    "while start_date<end_date:\n",
    "    get_comments('canada', end_date, end_date-86400)\n",
    "    end_date-=86400\n",
    "    count += 1\n",
    "    print(\"{} iterations, last end_date ={}\".format(count, end_date))\n",
    "\n",
    "#ran it a few time, stopped twice without request, X iter, 41 iterations\n",
    "#280 min on last one, 2,129,337 comments \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "#getting all the data at once is a bad idea lets build a loop that does it 86400 dt units at a time (1 day)\n",
    "#original_start_date = int(dt.datetime(2021, 1, 1, 0, 0).timestamp())\n",
    "#original_end_date = int(dt.datetime(2022, 1, 1, 0, 0).timestamp())\n",
    "start_date = int(dt.datetime(2021, 1, 1, 0, 0).timestamp())\n",
    "end_date = int(dt.datetime(2022, 1, 1, 0, 0).timestamp())\n",
    "count = 0\n",
    "\n",
    "while start_date<end_date:\n",
    "    get_comments('onguardforthee', end_date, end_date-86400)\n",
    "    end_date-=86400\n",
    "    count += 1\n",
    "    print(\"{} iterations, last end_date ={}\".format(count, end_date))\n",
    "\n",
    "#ran to completion, 141 min, 412,382 comments\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "#getting all the data at once is a bad idea lets build a loop that does it 86400 dt units at a time (1 day)\n",
    "#original_start_date = int(dt.datetime(2021, 1, 1, 0, 0).timestamp())\n",
    "#original_end_date = int(dt.datetime(2022, 1, 1, 0, 0).timestamp())\n",
    "start_date = int(dt.datetime(2021, 1, 1, 0, 0).timestamp())\n",
    "end_date = int(dt.datetime(2022, 1, 1, 0, 0).timestamp())\n",
    "count = 0\n",
    "\n",
    "while start_date<end_date:\n",
    "    get_comments('trollxchromosomes', end_date, end_date-86400)\n",
    "    end_date-=86400\n",
    "    count += 1\n",
    "    print(\"{} iterations, last end_date ={}\".format(count, end_date))\n",
    "\n",
    "#ran to completion in 78 min, got 186,416 comments\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting all the data at once is a bad idea lets build a loop that does it 86400 dt units at a time (1 day)\n",
    "#original_start_date = int(dt.datetime(2021, 1, 1, 0, 0).timestamp())\n",
    "#original_end_date = int(dt.datetime(2022, 1, 1, 0, 0).timestamp())\n",
    "start_date = 1619078398\n",
    "end_date = 1638864059\n",
    "count = 0\n",
    "\n",
    "while start_date<end_date:\n",
    "    get_comments('christianity', end_date, end_date-86400)\n",
    "    end_date-=86400\n",
    "    count += 1\n",
    "    print(\"{} iterations, last end_date ={}\".format(count, end_date))\n",
    "\n",
    "#ran to 1639209659"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creation of corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data from SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull_comments_from_SQL(subreddit):\n",
    "    engine = create_engine('mysql+pymysql://root:20RUNstackHost@localhost/{}'.format(subreddit))\n",
    "    df = pd.read_sql_table('comments', con=engine)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_to_remove = ['AutoModerator', 'dota2_responses_bot', 'HCE_Replacement_Bot', 'Kevin_Garnett_Bot', 'Rangers_Bot', 'DropBox_Bot', 'Website_Mirror_Bot', 'Metric_System_Bot', \\\n",
    "    'Fedora-Tip-Bot', 'Some_Bot', 'Brigade_Bot', 'Link_Correction_Bot', 'Porygon-Bot', 'KarmaConspiracy_Bot', 'SWTOR_Helper_Bot', 'annoying_yes_bot', 'wtf_content_bot', 'Insane_Photo_Bot', \\\n",
    "    'Antiracism_Bot', 'qznc_bot', 'mma_gif_bot', 'QUICHE-BOT', 'bRMT_Bot', 'hockey_gif_bot', 'nba_gif_bot', 'gifster_bot', 'imirror_bot', 'okc_rating_bot', 'tennis_gif_bot', 'nfl_gif_bot', \\\n",
    "    'CPTModBot', 'LocationBot', 'CreepySmileBot', 'FriendSafariBot', 'WritingPromptsBot', 'CreepierSmileBot', 'IAgreeBot', 'Cakeday-Bot', 'Meta_Bot', 'HockeyGT_Bot', 'soccer_gif_bot', \\\n",
    "    'gunners_gif_bot', 'xkcd_number_bot', 'GWHistoryBot', 'PokemonFlairBot', 'ChristianityBot', 'cRedditBot', 'StreetFightMirrorBot', 'FedoraTipAutoBot', 'UnobtaniumTipBot', 'astro-bot', \\\n",
    "    'TipMoonBot', 'PlaylisterBot', 'Wiki_Bot', 'fedora_tip_bot', 'GunnersGifsBot', 'PGN-Bot', 'GunnitBot', 'havoc_bot', 'Relevant_News_Bot', 'gfy_bot', 'RealtechPostBot', 'imgurHostBot', \\\n",
    "    'Gatherer_bot', 'JumpToBot', 'DeltaBot', 'Nazeem_Bot', 'PhoenixBot', 'AtheismModBot', 'IsItDownBot', 'malo_the_bot', 'RFootballBot', 'KSPortBot', 'Makes_Small_Text_Bot', 'CompileBot', \\\n",
    "    'SakuraiBot', 'asmrspambot', 'SurveyOfRedditBot', 'RfreebandzBOT', 'rule_bot', 'xkcdcomic_bot', 'PloungeMafiaVoteBot', 'PoliticBot', 'Dickish_Bot_Bot', 'SuchModBot', 'MultiFunctionBot', \\\n",
    "    'CasualMetricBot', 'xkcd_bot', 'VerseBot', 'BeetusBot', 'GameDealsBot', 'BadLinguisticsBot', 'rhiever-bot', 'gfycat-bot-sucksdick', 'chromabot', 'Readdit_Bot', 'wooshbot', \\\n",
    "    'disapprovalbot', 'request_bot', 'define_bot', 'dogetipbot', 'techobot', 'CaptionBot', 'rightsbot', 'colorcodebot', 'roger_bot', 'ADHDbot', 'hearing-aid_bot', 'WikipediaCitationBot', \\\n",
    "    'PonyTipBot', 'fact_check_bot', 'rusetipbot', 'test_bot0x00', 'classybot', 'NFLVideoBot', 'MAGNIFIER_BOT', 'WordCloudBot2', 'JotBot', 'WeeaBot', 'raddit-bot', 'comment_copier_bot', \\\n",
    "    'coinflipbot', 'VideoLinkBot', 'new_eden_news_bot', 'hwsbot', 'UrbanDicBot', 'hearingaid_bot', 'thankyoubot', 'GeekWhackBot', 'ExmoBot', 'CHART_BOT', 'tips_bot', 'GATSBOT', 'allinonebot', \\\n",
    "    'moderator-bot', 'rnfl_robot', 'StackBot', 'GooglePlusBot', 'hit_bot', 'randnumbot', 'CAH_BLACK_BOT', 'CalvinBot', 'DogeTipStatsBot', 'autourbanbot', 'GabenCoinTipBot', 'Definition_Bot', \\\n",
    "    'redditbots', 'redditreviewbot', 'bot', 'autowikibot', 'golferbot', 'topredditbot', 'c5bot', 'jerkbot-3hunna', 'gracefulclaritybot', 'valkyribot', 'gracefulcharitybot', 'ddlbot', \\\n",
    "    'NoSobStoryBot2', 'bitofnewsbot', 'conspirobot', 'tipmoonbot1', 'd3posterbot', 'serendipitybot', 'gabentipbot', 'givesafuckbot', 'SakuraiBot_test', 'ttumblrbots', 'haiku_robot', \\\n",
    "    'tipmoonbot2', 'MarciBestGirl', 'comfort_bot_1962', 'WaterIsWetBot', 'VRCbot', 'Prof_Acorn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_unwanted_authors(df):\n",
    "    '''\n",
    "    Removes all comments made by authors in the list \"authors_to_remove\" from the dataframe\n",
    "    '''\n",
    "    return df[~df['author'].isin(authors_to_remove)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_removed_and_deleted_comments(df):\n",
    "    '''\n",
    "    Removes all removed and deleted comments from the dataframe\n",
    "    '''\n",
    "    df = df[df.body != '[removed]']\n",
    "    return df[df.body != '[deleted]']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_negative_score_comments(df):\n",
    "    '''\n",
    "    Removes all comments with a negative score from the dataframe\n",
    "    '''\n",
    "    return df[df.score > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_comments_with_body_len_less_than(df, truncate_len=20):\n",
    "    '''\n",
    "    Removes all comments with a body length less than 20 (def) from the dataframe\n",
    "    '''\n",
    "    return df[df.body.str.len() > truncate_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_new_line_characters(df):\n",
    "    '''\n",
    "    Replaces all new line characters in the body of the dataframe with a space\n",
    "    '''\n",
    "    return df.replace(r'\\n', ' ', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_case_body(df):\n",
    "    '''\n",
    "    Converts all body text to lower case\n",
    "    '''\n",
    "    df.body = df.body.str.lower()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_links(string):\n",
    "    '''\n",
    "    Removes all links from the string\n",
    "    '''\n",
    "    pattern = re.compile('\\[(.*?)\\]\\(.*?\\)')\n",
    "    \n",
    "    return re.sub(pattern, '', string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_link_stragglers(df):\n",
    "    '''\n",
    "    Removes all link stragglers from the body of the dataframe in a gready way (drops all str containing http)\n",
    "    '''\n",
    "    return df[~df.body.str.contains('http')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_contractions(df):\n",
    "    '''\n",
    "    Removes all contractions from the dataframe\n",
    "    '''\n",
    "    df.body = df.body.apply(lambda x: contractions.fix(x))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_gibberish(df):\n",
    "    '''\n",
    "    Detects gibberish in the body of the comments\n",
    "    '''\n",
    "    df['gibberish'] = df.body.apply(lambda x: Detector.is_gibberish(x))\n",
    "    #drop rows where gibberish is detected\n",
    "    df = df[df.gibberish == False]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punct_and_non_alphabetic_caracters(string):\n",
    "    '''\n",
    "    Removes all punctuation and non alphabetic characters from the string\n",
    "    '''\n",
    "    words = word_tokenize(string)\n",
    "    words = [word for word in words if word.isalpha()]\n",
    "    string = ' '.join(words)\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_accented_chars(string):\n",
    "    '''\n",
    "    Removes all accented characters from the string\n",
    "    '''\n",
    "    return unicodedata.normalize('NFKD', string).encode('ascii', 'ignore').decode('utf-8', 'ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_with_spacy(string):\n",
    "    '''\n",
    "    Lemmatizes the body of the comments with spacy\n",
    "    '''\n",
    "    doc = nlp(string)\n",
    "    return \" \".join([token.lemma_ for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_elapsed(start_time):\n",
    "    return str(((time.time() - start_time) / 60).__round__(2)) + ' minutes elapsed total\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_of_comments_removed(df, last_shape):\n",
    "    return 'Number of comments removed: ' + str(last_shape - df.shape[0]) + '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comments_remaining_percentage(df, start_shape):\n",
    "    return 'Percentage of comments remaining: ' + str(((df.shape[0] / start_shape) * 100).__round__(2)) + '%\\n Total comments remaining: ' + str(df.shape[0]) + '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_spent_on_step(last_step_time):\n",
    "    return 'Time spent on last step: ' + str(((time.time() - last_step_time) / 60).__round__(2)) + ' minutes ' + str(((time.time()-last_step_time) % 60).__round__(2)) + ' seconds\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_df_column(df, truncate_at=20, no_bots=True, no_blanks=True, filter_neg_score=True, no_newline_char=True, no_links=True, lower=True, \\\n",
    "    no_accents=True, no_contractions=True, no_gibberish=True, no_punct_and_numbers=True, truncate=True, lemmatize=True):\n",
    "\n",
    "    #set up the start time and shape of the dataframe\n",
    "    start_shape = df.shape[0]\n",
    "    start_time = time.time()\n",
    "    last_shape = start_shape\n",
    "\n",
    "    print('Starting cleaning process... \\n Original number of comments: ' + str(start_shape) + '\\n')\n",
    "    \n",
    "    if no_bots:\n",
    "        last_step_time = time.time()\n",
    "        df = remove_unwanted_authors(df)\n",
    "        print('Bye bots o/\\n', num_of_comments_removed(df, last_shape), time_elapsed(start_time), time_spent_on_step(last_step_time), comments_remaining_percentage(df, start_shape))\n",
    "        last_shape = df.shape[0]\n",
    "\n",
    "    \n",
    "    if no_blanks:\n",
    "        last_step_time = time.time()\n",
    "        df = remove_removed_and_deleted_comments(df)\n",
    "        print('Removed & Deleted\\n', num_of_comments_removed(df, last_shape), time_elapsed(start_time), time_spent_on_step(last_step_time), comments_remaining_percentage(df, start_shape))\n",
    "        last_shape = df.shape[0]\n",
    "\n",
    "    if filter_neg_score:\n",
    "        last_step_time = time.time()\n",
    "        df = remove_negative_score_comments(df)\n",
    "        print('Remove Sub 1 scores\\n', num_of_comments_removed(df, last_shape), time_elapsed(start_time), time_spent_on_step(last_step_time), comments_remaining_percentage(df, start_shape))\n",
    "        last_shape = df.shape[0]\n",
    "\n",
    "    if no_newline_char:\n",
    "        last_step_time = time.time()\n",
    "        df = remove_new_line_characters(df)\n",
    "        print('Remove the new line character(s) \\n', time_elapsed(start_time), time_spent_on_step(last_step_time), comments_remaining_percentage(df, start_shape))\n",
    "        last_shape = df.shape[0]\n",
    "    \n",
    "    if no_links:\n",
    "        last_step_time = time.time()\n",
    "        df.body = df.body.apply(lambda x: remove_links(x))\n",
    "        print('Remove links in comments \\n', time_elapsed(start_time), time_spent_on_step(last_step_time), comments_remaining_percentage(df, start_shape))\n",
    "        df = remove_link_stragglers(df)\n",
    "        print('Remove straggler links \\n',  num_of_comments_removed(df, last_shape), time_elapsed(start_time), time_spent_on_step(last_step_time), comments_remaining_percentage(df, start_shape))\n",
    "        last_shape = df.shape[0]\n",
    "    \n",
    "    if lower:\n",
    "        last_step_time = time.time()\n",
    "        df = lower_case_body(df)\n",
    "        print('lower_case_body \\n', time_elapsed(start_time), time_spent_on_step(last_step_time), comments_remaining_percentage(df, start_shape))\n",
    "        last_shape = df.shape[0]\n",
    "    \n",
    "    if no_accents:\n",
    "        last_step_time = time.time()\n",
    "        df.body = df.body.apply(lambda x: remove_accented_chars(x))\n",
    "        print('remove_accented_chars \\n', time_elapsed(start_time), time_spent_on_step(last_step_time), comments_remaining_percentage(df, start_shape))\n",
    "        last_shape = df.shape[0]\n",
    "    \n",
    "    if no_contractions:\n",
    "        last_step_time = time.time()\n",
    "        df = clean_contractions(df)\n",
    "        print('clean_contractions:\\n', time_elapsed(start_time), time_spent_on_step(last_step_time), comments_remaining_percentage(df, start_shape))\n",
    "        last_shape = df.shape[0]\n",
    "    \n",
    "    if no_gibberish:\n",
    "        last_step_time = time.time()\n",
    "        df['gibberish'] = 0\n",
    "        df = detect_gibberish(df)\n",
    "        print('Del gibberish:\\n', num_of_comments_removed(df, last_shape), time_elapsed(start_time), time_spent_on_step(last_step_time), comments_remaining_percentage(df, start_shape))\n",
    "        last_shape = df.shape[0]\n",
    "    \n",
    "    if no_punct_and_numbers:\n",
    "        last_step_time = time.time()\n",
    "        df.body = df.body.apply(lambda x: remove_punct_and_non_alphabetic_caracters(x))\n",
    "        print('Remove punctuation & non alphabetic characters:\\n', time_elapsed(start_time), time_spent_on_step(last_step_time), comments_remaining_percentage(df, start_shape))\n",
    "        last_shape = df.shape[0]\n",
    "\n",
    "    if truncate:\n",
    "        last_step_time = time.time()\n",
    "        df = remove_comments_with_body_len_less_than(df, truncate_at)\n",
    "        print(f'Remove comments with # characters < {truncate_at}\\n', num_of_comments_removed(df, last_shape), time_elapsed(start_time), time_spent_on_step(last_step_time), comments_remaining_percentage(df, start_shape))\n",
    "        last_shape = df.shape[0]\n",
    "\n",
    "    if lemmatize:\n",
    "        last_step_time = time.time()\n",
    "        lem_count = 0\n",
    "        df.body = df.body.apply(lambda x: lemmatize_with_spacy(x))\n",
    "        print('finally done\\n', time_elapsed(start_time), time_spent_on_step(last_step_time), comments_remaining_percentage(df, start_shape))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_clean_comments(df, subreddit):\n",
    "    '''\n",
    "    Saves the cleaned comments to a csv file\n",
    "    ''' \n",
    "    current_time = dt.datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "    df.to_csv(f'Generated_Data/{subreddit}_clean_comments_{current_time}.csv', index=False)\n",
    "    print(f'Saved cleaned comments to {subreddit}_clean_comments_{current_time}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_corpus(df, subreddit):\n",
    "    '''\n",
    "    Saves the corpus to a csv file\n",
    "    '''\n",
    "    #current time in YYYY-MM-DD-HH-MM-SS format\n",
    "    current_time = dt.datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "    df = df.body\n",
    "    df.to_csv(f'Generated_Data/{subreddit}_corpus_{current_time}.csv', index=False)\n",
    "    print(f'Corpus saved to {subreddit}_corpus_{current_time}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All together now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_corpus(subreddit, truncate_at=20):\n",
    "    '''\n",
    "    Creates a corpus from a subreddit\n",
    "    does not pull comments from reddit live\n",
    "    all comments are pulled from the SQL database corresponding to the subreddit\n",
    "    '''\n",
    "    df = pull_comments_from_SQL(subreddit)\n",
    "    df = clean_df_column(df, truncate_at=truncate_at)\n",
    "    save_clean_comments(df, subreddit)\n",
    "    save_corpus(df, subreddit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### records of corpus generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "\n",
    "create_corpus('trollxchromosomes', truncate_at=20)\n",
    "#completed in 19 minutes, %82 of comments remaining\n",
    "\n",
    "\n",
    "create_corpus('onguardforthee', truncate_at=20)\n",
    "#  34.06 minutes elapsed total\n",
    "#  Time spent on last step: 30.11 minutes 6.34 seconds\n",
    "#  Percentage of comments remaining: 73.8206323263382%\n",
    "#  Total comments remaining: 304,423\n",
    "\n",
    "\n",
    "create_corpus('askscience', truncate_at=30)\n",
    "#  19.6 minutes elapsed total\n",
    "#  Time spent on last step: 17.21 minutes 12.35 seconds\n",
    "#  Percentage of comments remaining: 45.87917311819194%\n",
    "#  Total comments remaining: 135,116\n",
    "#  45% of comments were removed/deleted, highest by far\n",
    "\n",
    "\n",
    "create_corpus('atheism', truncate_at=30)\n",
    "# 108.27 minutes elapsed total\n",
    "# Time spent on last step: 95.37 minutes 22.13 seconds\n",
    "# Percentage of comments remaining: 76.63061839754937%\n",
    "# Total comments remaining: 948,097\n",
    "\n",
    "\n",
    "create_corpus('dota2', truncate_at=30)\n",
    "#  106.29 minutes elapsed total\n",
    "#  Time spent on last step: 94.73 minutes 43.92 seconds\n",
    "#  Percentage of comments remaining: 65.54133073549112%\n",
    "#  Total comments remaining: 1,141,030\n",
    "\n",
    "\n",
    "create_corpus('askreddit', truncate_at=40)\n",
    "#  116.96 minutes elapsed total\n",
    "#  Time spent on last step: 102.06 minutes 3.77 seconds\n",
    "#  Percentage of comments remaining: 55.71227948081808%\n",
    "#  Total comments remaining: 1,159,182\n",
    "\n",
    "\n",
    "create_corpus('twoxchromosomes', truncate_at=40)\n",
    "#  142.96 minutes elapsed total\n",
    "#  Time spent on last step: 126.23 minutes 14.06 seconds\n",
    "#  Percentage of comments remaining: 61.41970372721317%\n",
    "#  Total comments remaining: 994,705\n",
    "#  ~400k comments removed by moderator/deleted\n",
    "\n",
    "\n",
    "create_corpus('askwomen', truncate_at=40)\n",
    "# 154.01 minutes elapsed total\n",
    "# Time spent on last step: 134.61 minutes 36.65 seconds\n",
    "# Percentage of comments remaining: 69.02238164941976%\n",
    "# Total comments remaining: 1,059,067\n",
    "\n",
    "\n",
    "create_corpus('canada', truncate_at=40)\n",
    "#  211.27 minutes elapsed total\n",
    "#  Time spent on step: 191.95 minutes 56.79 seconds\n",
    "#  Percentage of comments remaining: 62.72534596449505%\n",
    "#  Total comments remaining: 1,335,634\n",
    "\n",
    "\n",
    "create_corpus('askmen', truncate_at=40)\n",
    "# 190.8 minutes elapsed total\n",
    "# Time spent on last step: 167.27 minutes 15.92 seconds\n",
    "# Percentage of comments remaining: 60.26%\n",
    "# Total comments remaining: 1642984\n",
    "# Saved cleaned comments to askmen_clean_comments_2022-01-21-22-28-23.csv\n",
    "# Corpus saved to askmen_corpus_2022-01-21-22-28-50.csv\n",
    "\n",
    "create_corpus('politics', truncate_at=40)\n",
    "#  245.88 minutes elapsed total\n",
    "#  Time spent on last step: 220.65 minutes 39.28 seconds\n",
    "#  Percentage of comments remaining: 62.15%\n",
    "#  Total comments remaining: 1779612\n",
    "# Saved cleaned comments to politics_clean_comments_2022-01-22-02-38-15.csv\n",
    "# Corpus saved to politics_corpus_2022-01-22-02-38-49.csv\n",
    "\n",
    "create_corpus('christianity', truncate_at=30)\n",
    " 198.54 minutes elapsed total\n",
    " Time spent on last step: 174.79 minutes 47.32 seconds\n",
    " Percentage of comments remaining: 72.23%\n",
    " Total comments remaining: 1286950\n",
    "Saved cleaned comments to christianity_clean_comments_2022-01-22-08-01-48.csv\n",
    "Corpus saved to christianity_corpus_2022-01-22-08-02-14.csv\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "415b51a014e6e0beb5722367d9da13b1dc115eed8d1c27ed06a75270ca49036a"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('NLP': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
